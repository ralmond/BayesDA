---
title: "Probability"
format: revealjs
---

```{r libs}
library(tidyverse)
library(plotly)
library(cowplot)
library(magick)
library(shiny)
library(vcdExtra)
library(DiagrammeR)
```

# Discrete Probabilities

## The Urn Model

```{r urnPlot}
urn_image <- "img/Amphora.png"
getwd()

ggdraw() + draw_image(urn_image)
```

-   Urn contains $b$ black balls and $w$ white balls.

-   All same size, weight, texture &c. Thoroughly mixed.

-   Draw a ball without looking.

Probability of drawing a black ball is $b/(b+w)$

## Probability: Frequency Definition

-   $E$ is an event (set of possible outcomes).

-   $N$ is the number of opportunities for $E$ to occur.

-   $C_N(E)$ is the number of times (count) that $E$ occurs in $N$ opportunities.

## $$ \Pr(E) = \lim_{N \rightarrow \infty} \frac{C_N(E)}{N} $$ Subjective Probability

The probability of an event $E$ is $p$ if **we** think it behaves like an urn with a proportion $p$ of black balls.

-   Gets around problems with event occurring only once.

-   Often used informally, "What is the probability it will rain this weekend?"

## The Law of large numbers.

Let $\delta$ be a small positive number.

Let $\Pr(E) = p_E$.

There exits an $N'$ for which if $N>N'$

$$ | p_E - C_E(N)/N 
 | < \delta $$

[Law of Large Numbers Demo](https://pluto.coe.fsu.edu/rdemos/IntroStats/LawofLargeNumbers.Rmd)

[Law of Large Numbers Animated Demo](https://pluto.coe.fsu.edu/rdemos/IntroStats/LawofLargeNumbersAnimated.Rmd)

## Subjective versus Objective

-   Probability is "objective" if **we** agree on its value.

    -   Document assumptions!

-   Probability is relative to state of information

-   Some people only accept frequency definition

-   Same people often make strong assumptions about likelihood with minimal documentation

*Models and assumptions should be checked!*

# Probability as a Measure

Like length, area, or count.

## Properties

Measures are a set function, map set (subset of $\Omega$) to non-negative real number.

-   $\mu(E) \ge 0$; $\mu(\emptyset)=0$.

-   $\mu(A \cup B) = \mu(A) + \mu(B)$ if $A \cap B = \emptyset$

    -   Extends to finite (& infinite) sums

Additionally probability:

-   $\overline{A} = \{\omega \in \Omega : \omega \not\in A \}$

-   $\Pr(A) + \Pr(\overline{A}) = 1$, therefore $\Pr(A) \leq 1$

## Additivity

-   Disjoint Events

    -   Urn has $g$ green, $r$ red and $w$ white balls.
    -   Probability of drawing a colored (green or red) ball: $$ \Pr(G \vee R) = \frac{g}{g+r+w} + \frac{r}{g+r+w}= \frac{g+r}{g+r+w} $$

-   Overlapping Events

    -   Need to know the degree of overlap, $\Pr (A \cap B)$

$$\Pr(A\cup B) = \Pr(A) + \Pr(B) - \Pr(A \cap B)$$

# Imprecision and Uncertainty

## Uncertainty versus Imprecision

-   *Uncertain* -- we don't know what will happen next time, but we can predict long run behavior

    -   Draw ball from urn of known composition

-   *Imprecise* -- we don't know (within limits) what will happen in the long run

    -   Draw ball from urn of unknown composition

-   Probability only deals with uncertainty, not imprecision.

    -   But sometimes imprecision is approximated with uncertainty.

## Imprecise Urn Models

-   Urn is population

-   Draws from urn are sample

-   Big source of problems in statistics is not being precise about who is in and is not in the urn

-   Imprecise:

    -   FSU graduate students

-   Precise:

    -   Students enrolled a Ph.D. or Masters program in the FSU College of Education on Sep 4, 2010.

## Chaotic Phenomenon with Unknown Starting values.

Coin flip is determined by initial linear and angular momentum, height of toss, moment of inertia of coin.

Small variations switch from "head" to "tails".

[Percy Diaconis Video](https://www.youtube.com/watch?v=AYnJv68T3MM&t=0s)

Often we model these unknown starting conditions as "random".

# Conditional Probability

## Definition

```{r CondProb, echo=FALSE}
plot(c(0,2),c(0,1),type="n",xaxt="n",yaxt="n",xlab="",ylab="")
polygon(c(0,0,1,1),c(0,1,1,0),col="blue")
polygon(c(1,1,2,2),c(0,1,1,0),col="yellow")
Ev.x <- c(.7,.7,1.2,1.2)
Ev.y <- c(.3,.7,.6,.4)
Ev.mf <-(1-Ev.x[1])/(Ev.x[4]-Ev.x[1])
Ev.mid <-(Ev.y[c(4,3)]-Ev.y[c(1,2)])*Ev.mf + Ev.y[c(1,2)]
polygon(c(Ev.x[1:2],c(1,1)),c(Ev.y[1:2],rev(Ev.mid)),
        col="aquamarine")
polygon(c(c(1,1),Ev.x[3:4]),c(Ev.mid,Ev.y[3:4]),
        col="yellowgreen")
abline(v=1)
text(.85,.5,"E")
axis(3,at=c(.5,1.5),labels=c("H","not H"),tick=FALSE)
```

$$\Pr(E|H) = \frac{\Pr(E \cap H)}{\Pr(H)} $$

## Examples

-   Pr ( Own a computer \| Graduate Student)

-   Pr( Amazon shopper buys Jeeves & Wooster \| Amazon shopper buys Complete Monte Python's Flying Circus)

-   Pr (Patient has fever \| Patient has strep throat)

-   Pr (Patient improves \| Patient given placebo)

## Law of Total Probability (Discrete Version)

$$ \Pr(E) = \Pr(E|H) \Pr(H) + \Pr(E|\overline{H})\Pr(\overline{H}) $$

$$ \Pr(B) = \sum_i \Pr(B|A_i)\Pr(A_i) $$

Where \$ A_i \cap A_j =\emptyset\$ and $\bigcup_i A_i = \Omega$

## Example

-   Pick a graduate student at random, what is probability student is in 2nd year of program

-   Distribution: Masters (60%), Ph.D. (40%)

-   Masters: 1st (50%), 2nd (40%), 3rd (10%)

-   PhD: 1st (25%), 2nd (25%), 3rd (20%), 4th (15%), 5th (8%), 6th (5%), 7th (2%)

```{r year}
program <- c(master=.6,doc=.4)
year.m <- c(.5,.4,.1,0,0,0,0)
year.p <- c(.25,.25,.20,.15,.08,.05,.02)
yearbyProg <- rbind(Masters=program[1]*year.m,Doc=program[2]*year.p)
colSums(yearbyProg)
```

## Bayes Theorem

$$ \Pr(H|E) = \frac{\Pr(E|H)\Pr(H)}{\Pr(E)} $$

$$ = \frac{\Pr(E|H)\Pr(H)}{\Pr(E|H)\Pr(H)+\Pr(E|\overline{H})\Pr(\overline{H})}$$

|            |                                  |
|------------|----------------------------------|
| Prior      | $\Pr(H)$                         |
| Likelihood | $\Pr(E|H)$;$\Pr(E|\overline{H})$ |
| Posterior  | $\Pr(H|E)$                       |

## Discrete IRT Example

-   *Skill* ($S$) can take on three values: `low`, `medium` and `high`
-   Prior distribution 1/3 of students expected in each cell.
-   Let $X$ be the score (right/wrong) for a single item.
    -   $\Pr(X=\text{right}|S=\text{high} = .9$
    -   $\Pr(X | S=\text{medium}) = .6$
    -   $\Pr(X | S=\text{low}) = .2$

```{r discreteIRT}
skill <- c(high=1/3,med=1/3,low=1/3)
like <- cbind(right=c(.9,.6,.2),wrong=c(.1,.4,.8))
rownames(like)<-names(skill)
probs <- sweep(like,1,skill,"*")
probs
```

```{r mosaicIRT}
mosaicplot(probs)
```

Posterior probability

```{r dirtpost}
probs[,1]/sum(probs[,1])
```

## Rare Disease Problem



# Independence

## Definition

```{r IndProb, echo=FALSE}
plot(c(0,2),c(0,1),type="n",xaxt="n",yaxt="n",xlab="",ylab="")
polygon(c(0,0,1,1),c(0,1,1,0),col="blue")
polygon(c(1,1,2,2),c(0,1,1,0),col="yellow")
Ev.x <- c(.75,.75,1.25,1.25)
Ev.y <- c(.4,.6,.6,.4)
Ev.mf <-(1-Ev.x[1])/(Ev.x[4]-Ev.x[1])
Ev.mid <-(Ev.y[c(4,3)]-Ev.y[c(1,2)])*Ev.mf + Ev.y[c(1,2)]
polygon(c(Ev.x[1:2],c(1,1)),c(Ev.y[1:2],rev(Ev.mid)),
        col="aquamarine")
polygon(c(c(1,1),Ev.x[3:4]),c(Ev.mid,Ev.y[3:4]),
        col="yellowgreen")
abline(v=1)
text(.85,.5,"B")
axis(3,at=c(.5,1.5),labels=c("A","not A"),tick=FALSE)
```

$$ \Pr(B) = \Pr(B|A) = \Pr(B|\overline{A}) $$ $$ \Pr(A) = \Pr(A|B) = \Pr(A|\overline{B}) $$ $$ \Pr(A \cap B) = \Pr(A|B)\Pr(B) = \Pr(A)\Pr(B) $$

-   Knowing $A$ provides no information about $B$ and vise versa.

## Accident Proneness (Feller, 1968)

-   Driving Skill: 5/6 Normal, 1/6 Accident Prone
-   Probability of accident in a given year
    -   1/100 Normal drivers
    -   1/10 Accident Prone drivers
-   Accidents happen independently each year.

## Calculate

$\Pr(Y_i)$. -- Prob of accident in a given year.

```{r PYear1}
DrivingSkill <- c(N=5/6,A=1/6)
AccLike <- cbind(Yes=c(N=1/100,A=1/10),No=c(N=99/100,A=9/10))
Year1 <- sweep(AccLike,1,DrivingSkill,"*")
Year1
sum(Year1[,"Yes"])

```

$\Pr(Y_1 \wedge Y_2)$ -- Accident in two years.

```{r PYear12}
Acc2Like <- AccLike
Acc2Like[,"Yes"] <- AccLike[,"Yes"]^2
Acc2Like[,"No"] <- 1 -Acc2Like[,"Yes"]
Year12 <- sweep(Acc2Like,1,DrivingSkill,"*")
Year12
sum(Year12[,"Yes"])
```

$\Pr(Y_2 | Y_1)$ -- Accident in 2nd year given accident in first year.

```{r PY1-Y2}
sum(Year12[,"Yes"])/sum(Year1[,"Yes"])
```

## Explanation

$\Pr(S=\text{normal}|A_i)$ -- Probability in normal category given accident.

```{r ciGraph}
DiagrammeR::grViz("
digraph AP {
  Driving -> Year1;
  Driving -> Year2;
}")
```

## Conditional Independence

-   *Conditional Independence*: \$\Pr(Y_1,Y_2\|S) = \Pr(Y_1\|S) \Pr(Y_2\|S) \$
-   Years are *marginally dependent*.
-   Separation in graph tells the story.
-   Information flows from from *Year1* to *Driving Skill* to *Year2*

## Another Example

```{r covidGraph}
DiagrammeR::grViz("
digraph Autotrain {
  Train -> COVID;
  Train -> MaskOnTrain;
  MaskOnTrain -> COVID;
  Vaccine -> COVID;
  COVID -> fever;
  COVID -> congestion;
  COVID -> pcrTest;
}")
```

## Differential Item Functioning

-   High stakes test we want it to be "fair" to certain group (focal group) as compared to reference group
    -   But, ability mix may be different in two groups
-   Let
-   $X_j$ be score on ItemÂ $j$
-   $A$ be ability of examinee
-   $G$ be group membership of examinee

Want $\Pr(X_i|A,G) = \Pr(X_i|A)$

## Gambler's Fallacy

-   For games of chance (e.g., spins of the roulette wheel) each event is independent

-   Seeing 10 heads in a row doesn't change probability of next toss

\*Law of large numbers is about infinity

-   But, unusual events makes us question our model

## Accident Pronness vs Gambler's Fallacy

Why doesn't the Gambler's Fallacy principle apply to the Accident Proneness example?

# Random Variables (Discrete)

## Definition

-   Imagine that each ball in the urn has a number painted on the side
-   Let $X(\omega)$ be the value of the number of the randomly chosen ball, *random variable*.
-   For Nominal/Ordinal variables, can use integer coding to create random variables.

*Convention*: Use captial letters for r.v.s, lower case for possible values.

## Probability (Mass) Function (p.f., p.m.f.)

-   Let $p(x)= \{\omega \in \Omega: X(\omega)=x\}$ be the proportion of balls which map to $x$.

-   Often shows with histogram

```{r barplot}
x <- 0:10
p <- dbinom(x,10,.3)
pmf <-data.frame(x=x,p=p)
ggplot(pmf,aes(x=x,y=p)) + geom_col()
```

## Cumulative Distribution Function (c.d.f. or d.f.)

$$ F(x) = \Pr(X\leq x) = \sum_{a < x} p(a) $$

-   *Support of* $X$ is set of values for which $p(x)>0$.
-   Distribution function is always non-decreasing.

```{r df}
pmf$Fx <- cumsum(pmf$p)
ggplot(pmf,aes(x=x,y=Fx)) + geom_step() + geom_point()
```

## Expected Value and Variance

*Expected Value* (mean)

$$ E[X] = \sum xp(x) = \mu $$ *Variance*

$$ \text{Var}(X) = E[(X-\mu)^2] = \sum (x-E[X])^2 p(x) $$ *Standard Deviation* $\sqrt{\text{Var}(X)}$ *Precision* $1/\text{Var}(X)$

## Moments

<https://en.wikipedia.org/wiki/Moment_(mathematics)>

## Linearity

$$E[aX+c] = aE[c] + c$$ $$\text{Var}(aX+c) = a^2\text{Var}(X)$$

# Some examples

-   [Uniform (discrete)](https://pluto.coe.fsu.edu/rdemos/IntroStats/DiscreteUniformParams.Rmd)
    -   All events are equally likely. (*At Random*)
-   [Binomial (& Bernoulli)](https://pluto.coe.fsu.edu/rdemos/IntroStats/BinomialParams.Rmd)
    -   Bernoulli trial is a single binary outcome.
    -   Binomial is the number of "successes" in a certain number of trials.
    -   Categorical and multinomial (more than two categories)
-   [Poisson](https://pluto.coe.fsu.edu/rdemos/IntroStats/PoissonParams.Rmd)
    -   Number of events in a given interval (of time or space)
    -   Events come at a uniform rate.
-   [Negative Binomial](https://pluto.coe.fsu.edu/rdemos/IntroStats/NegBinomial.Rmd)
    -   Number of trials until $k$ "successes."

#Law of Large Numbers

[Law Of Large Numbers](https://pluto.coe.fsu.edu/rdemos/IntroStats/LawOfLargeNumbers.Rmd)

[Law Of Large Numbers (Animated)](https://pluto.coe.fsu.edu/rdemos/IntroStats/LawOfLargeNumbersAnimated.Rmd)

## Bootstrap Distribution

-   Let $X_1, \ldots, X_N$ be an i.i.d. sample from unknown distribution $F(x)$.

-   Resample from from the sample with probability $1/N$ for $X=X_i$.

-   $F_N(x)$ is this empirical (bootstrap) distribution function

## Strong Law

As $N \rightarrow \infty$, $F_N(x) \rightarrow F(x)$

-   Recall $F(x) = \Pr(X \leq x)$

$$ F_n(x) = \text{count}(X_i < x)/N $$

By frequency definition, this should converge for every $x$.

## Statistics

A *statistic* is a *functional* which maps a distribution function to a summary.

A *functional* is an operator which maps a function to a (real) value.

By strong law of large numbers, sample statistic converges to population statistic.

# Continuous Variables

## The Area Model

-   Urn model breaks down when number of possible values is not countable.

-   Instead think of choosing a random point in a histogram

    -   Total area scaled to 1.

-   "Event" corresponds to an area in the histogram.

## Area model

```{r AreaModel}
plot(c(0,10),c(0,.15),type="n",xlab="x",ylab="f(x)",main="Uniform distribution over [0,10]")
polygon(c(0,0,10,10),c(0,.1,.1,0),col="lightblue")
polygon(c(3,3,4,4),c(0,.1,.1,0),col="red")
text(3.5,.125,"Pr (3 < X < 4)")
```

## Probability Density Function (p.d.f.)

-   Definition of (c).d.f. is still the same

-   p.m.f. is replaced by p.d.f. (probability density function), $f(x)$, height of curve at $x$.

$$F(x) = \int_{X\le x} f(X) dX $$

## CDFs and PDFs

```{r pdfanddf}
plot(c(-3,3),c(0,1),xlim=c(-3,3),type="n",xlab="x",ylab= "F(x) and f(x)")
curve(dnorm(x),add=TRUE)
xx <- c(-3,seq(-3,-1.5,.01),-1.5)
yy <- c(0,dnorm(seq(-3,-1.5,.01)),0)
polygon(xx,yy,col="lightblue")
curve(pnorm(x),add=TRUE)
text(c(2,2),c(.25,.75),c("f(x)","F(x)"),cex=2)
```

-   `pnorm` is normal distribution function.

-   `dnorm` is the density function

-   `qnorm` is the inverse distribution function (maps probability to value)

-   `rnorm`generates random numbers.

## Probability 0 and 1

-   What is $\Pr(x=\pi)$?

    -   This is a line, area is zero.

-   What is $\Pr(x \not = \pi)$?

    -   Whole space minus a line (area 0) is 1.

-   Probability zero means practically never

-   Probability one mean practically certain

    -   *Almost Surely*

-   But, an event of probability zero will occur

    -   In practice, always round.

## Lebesgue--Stieltjes Integrals

-   Integrals are the limits of adding up tiny rectangles (trapazoids).
-   Can define the base according to any measure:

Lebesgue--Stieltjes Integral: $E[h(x)] = \int h(x)dF(x)$

If measure is everywhere continuous: \$E\[h(x)\] = \int h(x)f(x)dx \$

If measure is counting measure $E[h(x)] = \sum h(x) p(x)$

Linear properties still hold.

# Common Continuous Distributions

-   [Uniform (Continuous)](https://pluto.coe.fsu.edu/rdemos/UniformParams.Rmd)

-   [Normal](https://pluto.coe.fsu.edu/rdemos/NormalParams.Rmd)

    -   [Log Normal](https://pluto.coe.fsu.edu/rdemos/LogNormalParams.Rmd)

-   [Gamma](https://pluto.coe.fsu.edu/rdemos/GammaParams.Rmd)

    -   Waiting times for Poisson Processes
    -   Can use either scale or rate (1/scale) parameter
    -   [Exponential](https://pluto.coe.fsu.edu/rdemos/ExpoentialParams.Rmd)
        -   Gamma distribution with shape parameter = 1.
    -   [Chi-squared](https://pluto.coe.fsu.edu/rdemos/Chi2Params.Rmd)
        -   Sum of the squares of $\nu$ unit normals; shape parameter is $\nu/2$.

-   [Beta](https://pluto.coe.fsu.edu/rdemos/BetaParams.Rmd)

    -   Values range between 0 and 1.
    -   Conjugate of binomial.
    -   Dirichlet (multivariate beta)
        -   Conjugate of multinomial.
        -   Defined on a simplex (a vector which must sum to one).

-   [Student's t](https://pluto.coe.fsu.edu/rdemos/StudenttParams.Rmd)

    -   Normal divided by square root of Chi-squared (with $\nu$ degrees of freedom).
    -   High kurtosis for low $\nu$.
    -   [Cauchy](https://pluto.coe.fsu.edu/rdmeos/CauchyParams.Rmd)
        -   Student's t with 1 degree of freedom
        -   Ratio of two normals.
        -   Mean and variance are infinite!

-   [Wishart](https://pluto.coe.fsu.edu/rdemos/Wishart.Rmd)

    -   Like Chi-square, but for covariance matrixes.
