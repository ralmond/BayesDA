---
title: "Introduction to MCMC"
format: revealjs
---

## Outline

1)  Monte Carlo Integration. (Stan Ulam)

2)  Rejection Sampling

3)  Metropolis--Hastings

4)  Gibbs Sampling

5)  Metropolis--Hastings--Green

## Monte Carlo Integration

Simple trick

Want to know $\int h(\theta) dF(\theta)$

$\theta$ may be multidimensional.

Let $\theta^{(1)}, \ldots, \theta^{(R)}$ be a sample of size $R$ from $F(\cdot)$; call its distribution $F_R(\theta)$.

$$ lim_{R\rightarrow\infty} \int h(\theta)dF_R(\theta) = \lim_{R\rightarrow\infty} \sum_{r=1}^R \frac{1}{R}h(\theta^{(r)}) = 
\int h(\theta) dF(\theta)$$

Approximation accuracy is related to $sd(\theta)/\sqrt{R}$

## Example, Emergency Cooling System.

-   The emergency cooling system for a nuclear reactor has 4 pumps.

    -   1 and 3 are on the left and 2 and 4 are on the right.
    -   At least one pump on each side needed in an emergence.

-   Let $\lambda$ be the average (geometric mean) of the probability of failure during an emergency.

    -   Let the median failure rate be .0001.
    -   Let the upper 97.5% bound be .001
    -   This is an *error factor* of 10.

```{r lambda dist}
lambda.mu <- log (.0001)
lambda.sigma <- (log(.001)-log(.0001))/2
cat("lambda ~ Lognormal(",round(lambda.mu,3),
    ",", round(lambda.sigma,3),")")
```

## Continued: different failure rates for the pumps.

Let $\lambda_1, \lambda_2, \lambda_3, \lambda_4$ be the failure rate of the four pumps.

$$\lambda_i = \phi_i \lambda ; \ \phi_i \sim LogNorm(0,log(2)/2)$$

(error factor of 2).

```{r phi}
phi.mu <- 0
phi.sigma <- log(2)/2
cat("phi ~ Lognormal(",round(phi.mu,3),
    ",", round(phi.sigma,3),")")
```

## Simulation

```{r}
R <- 1000
h <- function (lam) {
  1-(1-lam[1]*lam[3])*(1-lam[2]*lam[4])
}
randPumps <- function(lam.mu, lam.sig, phi.mu, phi.sig, npumps) {
  lambda <- rlnorm(1,lam.mu,lam.sig)
  phi <- rlnorm(npumps,phi.mu,phi.sig)
  lambda*phi
}
sample <- sapply(1:R, function (r) 
  h(randPumps(log(.001),lambda.sigma,
              phi.mu,phi.sigma,4)))
hist(sample)
round(mean(sample),9)
round(median(sample),9)
h(rep(.0001,4))
```

## Rejection Sampling

Want to draw samples from $f(\cdot)$, which is hard to sample from.

Pick another distribution $g(\cdot)$ which is easy to sample from.

Pick $M$ such that $\frac{f(x)}{Mg(x)}$ is always less than 1.

-   For $r=1, \ldots, R$:
    -   Draw $Y^{(*)}$ from $g(x)$
    -   Draw $u$ from a unit uniform.
    -   If $u < \frac{f(x)}{Mg(x)}$ then $X^{(r)} = Y^{(*)}$.
    -   Else, repeat.

Need to draw $MR$ values for $Y$ and $u$ (on average) to get $R$ samples for $X$.

## Example, drawing from a gamma distribution.

Gamma distributions with (small) integer parameters is easy: sum of exponentials.

Gamma distributions with non-integer parameters is hard.

$X \sim Gamma(3.5,1)$ Proposal $Y \sim Gamma(3,1)$.

```{r GammaDist}
curve(dgamma(x,3.5),xlim=c(0,10))
curve(dgamma(x,3),add=TRUE,lty=2)
```

Pick an M.

```{r GammaDist}
M <- 1.5
curve(dgamma(x,3.5),xlim=c(0,10))
curve(M*dgamma(x,3),add=TRUE,lty=2)
xx <- 5
segments(c(xx,xx),c(0,dgamma(xx,3)),
         c(xx,xx),c(dgamma(xx,3),M*dgamma(xx,3.5)),
         lwd=c(1.2,1.0),col=c("red","cyan"))
```

```{r samGamma}
sampleGamma <- function (shape=3.5,
                         k = floor(shape),
                         M=1.5) {
  Y <- sum(rexp(k))
  u <- runif(1)
  if (M*u < dgamma(Y,shape)/dgamma(Y,k)) 
    return (Y)
  else 
    sampleGamma(shape,k,M)
}
N <- 1000
gammaSamp <- sapply(1:N,function(r) sampleGamma())
mean(gammaSamp)
sd(gammaSamp)
qqplot(qgamma((1:N)/(N+1),3.5),gammaSamp)
```

## Metropolis et al.

Metropolis, Rosenbluth, Rosenbluth, Teller and Teller.

Track movement of a gas.

Current position is $x^{(r)}$

-   Propose a new position $y$ with probability $g(y|x)$.
    -   Uniform random number around $x$.
    -   Normal random number around $x$
    -   Step size (width of uniform or ball) is tuning parameter.
-   Calculate Acceptance Ratio $$ \alpha = f(y)/f(x^{(r)})$$
    -   If $u < \alpha$, $x^{(r+1)} = y$
    -   else $x^{(r+1)} = x^{(r)}$

$x^{(b)}, x^{(b+1)}, \ldots$ are distributed according to $f(\cdot)$.

## Metropolis--Hastings

## Gibbs Sampling

## Autocorrelation

Autocorrelation lag $\ell$ is 
$\text{Cor}(X_t,X_{t+\ell})$

`acf` is autocorrelation function.

Effective Sample Size (ESS) is related to autocorrelation.

Two sample sizes:
  - $N$ -- how much data
  - $R$ -- size of the MC sample
  


## Gibbs is not better than Metropolis

## Dealing with changing parameter space
